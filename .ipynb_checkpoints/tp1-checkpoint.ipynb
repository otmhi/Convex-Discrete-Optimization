{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 HomeWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Optimisation without constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the implementation of the gradient descent algorithm with a constant stepsize, we try to study its convergence with different step sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otmane/Desktop/3A/centrale/opt/tp/main.py:55: RuntimeWarning: invalid value encountered in subtract\n",
      "  xnp1=xn-rho*dfx # nouveau point courant (x_{n+1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0.1, did the algorithm converge?  False\n",
      "for 0.05, did the algorithm converge?  False\n",
      "for 0.01, did the algorithm converge?  True\n",
      "for 0.001, did the algorithm converge?  True\n"
     ]
    }
   ],
   "source": [
    "rhos = [0.1, 0.05, 0.01, 0.001]\n",
    "for rho in rhos :\n",
    "    results = gradient_rho_constant(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))\n",
    "    print('for '+ str(rho) +', did the algorithm converge? ', results['converged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for high values of rho, the algorithm does not converge, it even results on invalid values for the update rule ( xnp1 = inf ). So we either reduce the stepsize parameter or define an adaptative algorithm that can handle this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **We define an adaptative version of the gradient descend algorithm** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_rho_adaptatif(fun, fun_der, U0, rho, tol,args):\n",
    "    \n",
    "    # Fonction permettant de minimiser la fonction f(U) par rapport au vecteur U \n",
    "    # Méthode : gradient à pas fixe\n",
    "    # INPUTS :\n",
    "    # - han_f   : handle vers la fonction à minimiser\n",
    "    # - han_df  : handle vers le gradient de la fonction à minimiser\n",
    "    # - U0      : vecteur initial \n",
    "    # - rho     : paramètre gérant l'amplitude des déplacement \n",
    "    # - tol     : tolérance pour définir le critère d'arrêt\n",
    "    # OUTPUT : \n",
    "    # - GradResults : structure décrivant la solution\n",
    "\n",
    "\n",
    "    itermax=10000  # nombre maximal d'itérations \n",
    "    xn=U0\n",
    "    f=fun(xn,*args) # point initial de l'algorithme\n",
    "    it=0         # compteur pour les itérations\n",
    "    converged = False;\n",
    "    \n",
    "    while (~converged & (it < itermax)):\n",
    "        it=it+1\n",
    "        dfx=fun_der(xn,*args)# valeur courante de la fonction à minimiser\n",
    "        \n",
    "        xnp1=xn-rho*dfx\n",
    "        fnp1 = fun(xnp1,*args)\n",
    "        \n",
    "        if fnp1 < f :\n",
    "            \n",
    "            if abs(fnp1-f)<tol:\n",
    "                converged = True\n",
    "                \n",
    "            xn, f = xnp1, fnp1\n",
    "            rho *= 2\n",
    "        else :\n",
    "            rho /= 2\n",
    "\n",
    "    GradResults = {\n",
    "            'initial_x':U0,\n",
    "            'minimum':xnp1,\n",
    "            'f_minimum':fnp1,\n",
    "            'iterations':it,\n",
    "            'converged':converged\n",
    "            }\n",
    "    return GradResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see if both versions give us the same minimum or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum value we reach with a constant stepsize: -1.8368912354010196\n",
      "the minimum value we reach with an adaptative stepsize: -1.817778210582762\n"
     ]
    }
   ],
   "source": [
    "res_gd_const = gradient_rho_constant(f1,df1,x0,rho=0.01,tol=1e-6,args=(B,S)) \n",
    "res_gd_adaptative = gradient_rho_adaptatif(f1,df1,x0,rho=0.01,tol=1e-6,args=(B,S)) \n",
    "print('the minimum value we reach with a constant stepsize:', res_gd_const['f_minimum'])\n",
    "print('the minimum value we reach with an adaptative stepsize:', res_gd_adaptative['f_minimum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the same stepsize, the simple version reaches a better minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the adaptative version can deal with the convergence issue discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 1, did the algorithme converge?  True\n",
      "for 0.1, did the algorithme converge?  True\n",
      "for 0.05, did the algorithme converge?  True\n",
      "for 0.01, did the algorithme converge?  True\n",
      "for 0.001, did the algorithme converge?  True\n"
     ]
    }
   ],
   "source": [
    "rhos = [1, 0.1, 0.05, 0.01, 0.001]\n",
    "for rho in rhos :\n",
    "    results = gradient_rho_adaptatif(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))\n",
    "    print('for '+ str(rho) +', did the algorithme converge? ', results['converged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indeed, it can converge for any stepsize we choose.**\n",
    "\n",
    "Another great benefit of using the adaptative version is the **speed of convergence**. We can further compare both algorithms by ploting the number of iterations required for the convergence of both versions depending of the stepsize we pick. For instance, we take values for which both algorithms can converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rhos = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "adaptative = [gradient_rho_adaptatif(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))['iterations'] for rho in rhos]\n",
    "constant = [gradient_rho_constant(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))['iterations'] for rho in rhos]\n",
    "\n",
    "plt.plot(adaptative, label= 'adaptative')\n",
    "plt.plot(constant, label = 'constant')\n",
    "plt.title('Comparaison of the speed of convergence for both algorithms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two algorithms make one call of the objective function in an iteration, and perform basic arithmetic operations inside the iterations loop. The complexity of such algorithms is **O(n)** with n the number of iterations. All the comparaisons then boil down to one criterion which is the convergence speed.\n",
    "\n",
    "As we know, the speed of convergence of the simple gradient descent is highly affected by the value of the stepsize we choose. The adaptative version however have a constant convergence speed and does not depend on the value of the stepsize. \n",
    "\n",
    "So if we want to conclude, we can say that in terms of convergence and speed, the adaptative version outperforms the constant one, but the simple gradient descent can give better minimas if we use a suitable stepsize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Quasi-Newton methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare now the performance of the BFGS method with the algorithms we implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -1.836962311965238\n",
       " hess_inv: array([[ 0.5004328 , -0.21223272,  0.05691395, -0.28056012,  0.50919827],\n",
       "       [-0.21223272,  0.22383377,  0.0370393 ,  0.11542843, -0.34043246],\n",
       "       [ 0.05691395,  0.0370393 ,  0.1247386 , -0.04905887, -0.09108766],\n",
       "       [-0.28056012,  0.11542843, -0.04905887,  0.20504998, -0.28961765],\n",
       "       [ 0.50919827, -0.34043246, -0.09108766, -0.28961765,  0.77691404]])\n",
       "      jac: array([-2.98023224e-08, -8.94069672e-08,  1.49011612e-08, -1.49011612e-08,\n",
       "        1.49011612e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 91\n",
       "      nit: 10\n",
       "     njev: 13\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.69603139,  0.15793134, -0.61407083,  0.49414155, -0.05345803])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfgs_results = minimize(fun = f1, x0 = x0, args=(B,S), method='BFGS', tol=1e-6)\n",
    "bfgs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of the BFGS method is phenomenal, it did converge after **10** iterations and a gave us a slighly better minima than our gradient descent implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closed form of the minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function we have is a simple quadratic function, so an analytical solution can be easily found. It's Hessian matrix is **S** which is positive definite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45.48200751, 14.49627115,  0.34399006,  1.89569112,  7.21954015])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvals(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues of S are strictly positive, so our function is convex and an analytical solution of the minimum can be found and it's the value that sets the first derivative to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8369623119652505"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_1 = np.linalg.inv(S)\n",
    "minimum = 0.5*S_1*B\n",
    "f_minimum = f1(minimum, B, S)\n",
    "f_minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm the performances of the quasi-newton methods, we can check the relative absolute difference between the analytical minimum and the result of the BFGS method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7690544301362755e-15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs((f_minimum - bfgs_results['fun'])/f_minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the BFGS method is hugely precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimisation under constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Scipy method usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following results, we're gonna try to find the minimum of our functions in $ U = [0, 1]^5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the f2 function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(U,S):\n",
    "    n=U.shape[0]\n",
    "    U=np.matrix(U)\n",
    "    U.shape=(n,1)\n",
    "    fU = np.transpose(U) * S * U + np.transpose(U) * np.exp(U);\n",
    "    return float(fU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **Sequential Quadratic Programming** method to minimise both functions $f_1$ and $f_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqp_f1 = minimize(fun = f1, x0 = x0, args=(B,S), method='SLSQP', bounds=[(0,1)]*5, tol=1e-6)\n",
    "sqp_f2 = minimize(fun = f2, x0 = x0, args=(S), method='SLSQP', bounds=[(0,1)]*5, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the argmins and minimums for both function are: \n",
      "\n",
      "f1: argmin = [0.      0.12698 0.      0.01945 0.     ] f_min = -0.13853 \n",
      "\n",
      "f2: argmin = [0. 0. 0. 0. 0.] f_min = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('the argmins and minimums for both function are: \\n')\n",
    "print('f1: argmin =', np.round(sqp_f1['x'], 5), 'f_min =', np.round(sqp_f1['fun'], 5), '\\n')\n",
    "print('f2: argmin =', np.round(sqp_f2['x'], 5), 'f_min =', np.round(sqp_f2['fun'], 5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both solutions respect the boundaries (constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Optimisation under constraints and penalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple and classical penalisation function that is $C^{\\infty}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalisation for x0: 0.0\n",
      "Penalisation for x0 + 2: 5.0\n",
      "Penalisation for x0 - 1: 5.0\n"
     ]
    }
   ],
   "source": [
    "Beta = lambda u : np.sum(np.maximum(u-1, 0)**2 + (np.maximum(-u,0))**2)\n",
    "print('Penalisation for x0:', Beta(x0))\n",
    "print('Penalisation for x0 + 2:', Beta(x0 + 1))\n",
    "print('Penalisation for x0 - 1:', Beta(x0 - 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the penalisation method as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1/2#The start values for epsilon\n",
    "decay = 2 #The decay parameter for epsilon in each iteration\n",
    "num_iter = 1000 #number of iterations\n",
    "x_1, x_2 = x0, x0 # the starting point for both functions\n",
    "\n",
    "for k in range(num_iter):\n",
    "    f1_penal, f2_penal = lambda U : f1(U,B,S) + (1/epsilon)*Beta(U), lambda U : f2(U,S) + (1/epsilon)*Beta(U)\n",
    "    x_1 = minimize(fun = f1_penal, x0 = x_1, method='BFGS', tol=1e-6)['x']\n",
    "    x_2 = minimize(fun = f2_penal, x0 = x_2, method='BFGS', tol=1e-6)['x']\n",
    "    epsilon /= decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the argmins and minimums for both function are: \n",
      "\n",
      "f1: argmin = [0.      0.12689 0.      0.01941 0.     ] f_min = -0.13853 \n",
      "\n",
      "f2: argmin = [0. 0. 0. 0. 0.] f_min = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('the argmins and minimums for both function are: \\n')\n",
    "print('f1: argmin =', np.round(x_1, 5), 'f_min =', np.round(f1(x_1,B,S), 5), '\\n')\n",
    "print('f2: argmin =', np.round(x_2, 5), 'f_min =', np.round(f2(x_2,S), 5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both methods give close results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Dual methods for optimisation under constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Uzawa algorithm, we need to define the Lagragian function and it's derivative. We know that our constraints are affine functions so the dual gap is equal to zero. We can surely obtain the true minimum by applying this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagrangian implementation \n",
    "\n",
    "def Lagragian1(U,p):\n",
    "    low_bound, up_bound = -U, U-1\n",
    "    return f1(U,B,S) + np.sum(p*np.concatenate([low_bound, up_bound], axis=0))\n",
    "    \n",
    "\n",
    "def dLagrangian1(U,p):\n",
    "    low_bound, up_bound = -U, U-1\n",
    "    return np.concatenate([low_bound, up_bound], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Uzawa algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uzawa(Lagrangian, dL, p0, x0, rho, iterations):\n",
    "    p_max, x_min = p0, x0\n",
    "    for i in range(iterations) : \n",
    "        x_min = minimize(fun = Lagrangian, x0 = x_min, args= (p_max), method='BFGS', tol=1e-6)['x']\n",
    "        p_max = np.maximum(0 , p_max + (rho)*dL(x_min,p_max))\n",
    "    return {'x_min': x_min, 'p_max': p_max, 'l_min': Lagragian1(x_min, p_max)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.zeros(10)\n",
    "results = uzawa(Lagrangian=Lagragian1, dL=dLagrangian1 , p0=p0 , x0=x0, rho=1, iterations=1000)\n",
    "x_min, l_min = results['x_min'], results['l_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrangian: argmaxmin = [-0.       0.12689 -0.       0.01941 -0.     ] l_min = -0.13853 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lagrangian: argmaxmin =', np.round(x_min, 5), 'l_min =', np.round(l_min, 5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our claims are confirmed and there is no duality gap. the solution for the dual problem matches the first solutions we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
