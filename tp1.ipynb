{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Optimisation without constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the implementation of the gradient descent algorithm with a constant stepsize, we try to study its convergence with different step sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/otmane/Desktop/3A/centrale/opt/tp/main.py:57: RuntimeWarning: invalid value encountered in subtract\n",
      "  xnp1=xn-rho*dfx # nouveau point courant (x_{n+1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0.1, did the algorithm converge?  False\n",
      "for 0.05, did the algorithm converge?  False\n",
      "for 0.01, did the algorithm converge?  True\n",
      "for 0.001, did the algorithm converge?  True\n"
     ]
    }
   ],
   "source": [
    "rhos = [0.1, 0.05, 0.01, 0.001]\n",
    "for rho in rhos :\n",
    "    results = gradient_rho_constant(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))\n",
    "    print('for '+ str(rho) +', did the algorithm converge? ', results['converged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for high values of rho, the algorithm does not converge, it even results on invalid values for the update rule ( xnp1 = inf ). So we either reduce the stepsize parameter or define an adaptative algorithm that can handle this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **We define an adaptative version of the gradient descend algorithm** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_rho_adaptatif(fun, fun_der, U0, rho, tol,args):\n",
    "    \n",
    "    # Fonction permettant de minimiser la fonction f(U) par rapport au vecteur U \n",
    "    # Méthode : gradient à pas fixe\n",
    "    # INPUTS :\n",
    "    # - han_f   : handle vers la fonction à minimiser\n",
    "    # - han_df  : handle vers le gradient de la fonction à minimiser\n",
    "    # - U0      : vecteur initial \n",
    "    # - rho     : paramètre gérant l'amplitude des déplacement \n",
    "    # - tol     : tolérance pour définir le critère d'arrêt\n",
    "    # OUTPUT : \n",
    "    # - GradResults : structure décrivant la solution\n",
    "\n",
    "\n",
    "    itermax=10000  # nombre maximal d'itérations \n",
    "    xn=U0\n",
    "    f=fun(xn,*args) # point initial de l'algorithme\n",
    "    it=0         # compteur pour les itérations\n",
    "    converged = False;\n",
    "    \n",
    "    while (~converged & (it < itermax)):\n",
    "        it=it+1\n",
    "        dfx=fun_der(xn,*args)# valeur courante de la fonction à minimiser\n",
    "        \n",
    "        xnp1=xn-rho*dfx\n",
    "        fnp1 = fun(xnp1,*args)\n",
    "        \n",
    "        if fnp1 < f :\n",
    "            \n",
    "            if abs(fnp1-f)<tol:\n",
    "                converged = True\n",
    "                \n",
    "            xn, f = xnp1, fnp1\n",
    "            rho *= 2\n",
    "        else :\n",
    "            rho /= 2\n",
    "\n",
    "    GradResults = {\n",
    "            'initial_x':U0,\n",
    "            'minimum':xnp1,\n",
    "            'f_minimum':fnp1,\n",
    "            'iterations':it,\n",
    "            'converged':converged\n",
    "            }\n",
    "    return GradResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see if both versions give us the same minimum or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum value we reach with a constant stepsize: -1.8368912354010196\n",
      "the minimum value we reach with an adaptative stepsize: -1.817778210582762\n"
     ]
    }
   ],
   "source": [
    "res_gd_const = gradient_rho_constant(f1,df1,x0,rho=0.01,tol=1e-6,args=(B,S)) \n",
    "res_gd_adaptative = gradient_rho_adaptatif(f1,df1,x0,rho=0.01,tol=1e-6,args=(B,S)) \n",
    "print('the minimum value we reach with a constant stepsize:', res_gd_const['f_minimum'])\n",
    "print('the minimum value we reach with an adaptative stepsize:', res_gd_adaptative['f_minimum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the same stepsize, the simple version reaches a better minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the adaptative version can deal with the convergence issue discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 1, did the algorithme converge?  True\n",
      "for 0.1, did the algorithme converge?  True\n",
      "for 0.05, did the algorithme converge?  True\n",
      "for 0.01, did the algorithme converge?  True\n",
      "for 0.001, did the algorithme converge?  True\n"
     ]
    }
   ],
   "source": [
    "rhos = [1, 0.1, 0.05, 0.01, 0.001]\n",
    "for rho in rhos :\n",
    "    results = gradient_rho_adaptatif(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))\n",
    "    print('for '+ str(rho) +', did the algorithme converge? ', results['converged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indeed, it can converge for any stepsize we choose.**\n",
    "\n",
    "Another great benefit of using the adaptative version is the **speed of convergence**. We can further compare both algorithms by ploting the number of iterations required for the convergence of both versions depending of the stepsize we pick. For instance, we take values for which both algorithms can converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rhos = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "adaptative = [gradient_rho_adaptatif(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))['iterations'] for rho in rhos]\n",
    "constant = [gradient_rho_constant(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))['iterations'] for rho in rhos]\n",
    "\n",
    "plt.plot(adaptative, label= 'adaptative')\n",
    "plt.plot(constant, label = 'constant')\n",
    "plt.title('Comparaison of the speed of convergence for both algorithms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two algorithms make one call of the objective function in an iteration, and perform basic arithmetic operations inside the iterations loop. The complexity of such algorithms is **O(n)** with n the number of iterations. All the comparaisons then boil down to one criterion which is the convergence speed.\n",
    "\n",
    "As we know, the speed of convergence of the simple gradient descent is highly affected by the value of the stepsize we choose. The adaptative version however have a constant convergence speed and does not depend on the value of the stepsize. \n",
    "\n",
    "So if we want to conclude, we can say that in terms of convergence and speed, the adaptative version outperforms the constant one, but the simple gradient descent can give better minimas if we use a suitable stepsize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Quasi-Newton methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare now the performance of the BFGS method with the algorithms we implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -1.836962311965238\n",
       " hess_inv: array([[ 0.5004328 , -0.21223272,  0.05691395, -0.28056012,  0.50919827],\n",
       "       [-0.21223272,  0.22383377,  0.0370393 ,  0.11542843, -0.34043246],\n",
       "       [ 0.05691395,  0.0370393 ,  0.1247386 , -0.04905887, -0.09108766],\n",
       "       [-0.28056012,  0.11542843, -0.04905887,  0.20504998, -0.28961765],\n",
       "       [ 0.50919827, -0.34043246, -0.09108766, -0.28961765,  0.77691404]])\n",
       "      jac: array([-2.98023224e-08, -8.94069672e-08,  1.49011612e-08, -1.49011612e-08,\n",
       "        1.49011612e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 91\n",
       "      nit: 10\n",
       "     njev: 13\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.69603139,  0.15793134, -0.61407083,  0.49414155, -0.05345803])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfgs_results = minimize(fun = f1, x0 = x0, args=(B,S), method='BFGS', tol=1e-6)\n",
    "bfgs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of the BFGS method is phenomenal, it did converge after **10** iterations and a gave us a slighly better minima than our gradient descent implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closed form of the minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function we have is a simple quadratic function, so an analytical solution can be easily found. It's Hessian matrix is **S** which is positive definite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45.48200751, 14.49627115,  0.34399006,  1.89569112,  7.21954015])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvals(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues of S are strictly positive, so our function is convex and an analytical solution of the minimum can be found by settinh the first derivative to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the analytical minimum is : -1.8369623119652505\n"
     ]
    }
   ],
   "source": [
    "S_1 = np.linalg.inv(S)\n",
    "minimum = 0.5*S_1*B\n",
    "f_minimum = f1(minimum, B, S)\n",
    "print('the analytical minimum is :', f_minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm the performances of the quasi-newton methods, we can check the relative absolute difference between the analytical minimum and the result of the BFGS method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7690544301362755e-15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs((f_minimum - bfgs_results['fun'])/f_minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the BFGS method is hugely precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimisation under constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Scipy method usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following results, we're gonna try to find the minimum of our functions in $ U = [0, 1]^5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the f2 function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(U,S):\n",
    "    n=U.shape[0]\n",
    "    U=np.matrix(U)\n",
    "    U.shape=(n,1)\n",
    "    fU = np.transpose(U) * S * U + np.transpose(U) * np.exp(U);\n",
    "    return float(fU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **Sequential Quadratic Programming** method to minimise both functions $f_1$ and $f_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqp_f1 = minimize(fun = f1, x0 = x0, args=(B,S), method='SLSQP', bounds=[(0,1)]*5, tol=1e-6)\n",
    "sqp_f2 = minimize(fun = f2, x0 = x0, args=(S), method='SLSQP', bounds=[(0,1)]*5, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the argmins and minimums for both function are: \n",
      "\n",
      "f1: argmin = [0.      0.12698 0.      0.01945 0.     ] f_min = -0.13853 \n",
      "\n",
      "f2: argmin = [0. 0. 0. 0. 0.] f_min = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('the argmins and minimums for both function are: \\n')\n",
    "print('f1: argmin =', np.round(sqp_f1['x'], 5), 'f_min =', np.round(sqp_f1['fun'], 5), '\\n')\n",
    "print('f2: argmin =', np.round(sqp_f2['x'], 5), 'f_min =', np.round(sqp_f2['fun'], 5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both solutions respect the boundaries (constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Optimisation under constraints and penalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple and classical penalisation function that is $C^{\\infty}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalisation for x0: 0.0\n",
      "Penalisation for x0 + 2: 5.0\n",
      "Penalisation for x0 - 1: 5.0\n"
     ]
    }
   ],
   "source": [
    "Beta = lambda u : np.sum(np.maximum(u-1, 0)**2 + (np.maximum(-u,0))**2)\n",
    "print('Penalisation for x0:', Beta(x0))\n",
    "print('Penalisation for x0 + 2:', Beta(x0 + 1))\n",
    "print('Penalisation for x0 - 1:', Beta(x0 - 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the penalisation method as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1/2#The start values for epsilon\n",
    "decay = 2 #The decay parameter for epsilon in each iteration\n",
    "num_iter = 1000 #number of iterations\n",
    "x_1, x_2 = x0, x0 # the starting point for both functions\n",
    "\n",
    "for k in range(num_iter):\n",
    "    f1_penal, f2_penal = lambda U : f1(U,B,S) + (1/epsilon)*Beta(U), lambda U : f2(U,S) + (1/epsilon)*Beta(U)\n",
    "    x_1 = minimize(fun = f1_penal, x0 = x_1, method='BFGS', tol=1e-6)['x']\n",
    "    x_2 = minimize(fun = f2_penal, x0 = x_2, method='BFGS', tol=1e-6)['x']\n",
    "    epsilon /= decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the argmins and minimums for both function are: \n",
      "\n",
      "f1: argmin = [0.      0.12689 0.      0.01941 0.     ] f_min = -0.13853 \n",
      "\n",
      "f2: argmin = [0. 0. 0. 0. 0.] f_min = 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('the argmins and minimums for both function are: \\n')\n",
    "print('f1: argmin =', np.round(x_1, 5), 'f_min =', np.round(f1(x_1,B,S), 5), '\\n')\n",
    "print('f2: argmin =', np.round(x_2, 5), 'f_min =', np.round(f2(x_2,S), 5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both methods give close results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Dual methods for optimisation under constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Uzawa algorithm, we need to define the Lagragian function and it's derivative. We know that our constraints are affine functions so the dual gap is equal to zero. We can surely obtain the true minimum by applying this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagrangian implementation \n",
    "\n",
    "def Lagragian1(U,p):\n",
    "    low_bound, up_bound = -U, U-1\n",
    "    return f1(U,B,S) + np.sum(p*np.concatenate([low_bound, up_bound], axis=0))\n",
    "    \n",
    "\n",
    "def dLagrangian1(U,p):\n",
    "    low_bound, up_bound = -U, U-1\n",
    "    return np.concatenate([low_bound, up_bound], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Uzawa algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uzawa(Lagrangian, dL, p0, x0, rho, iterations):\n",
    "    p_max, x_min = p0, x0\n",
    "    for i in range(iterations) : \n",
    "        x_min = minimize(fun = Lagrangian, x0 = x_min, args= (p_max), method='BFGS', tol=1e-6)['x']\n",
    "        p_max = np.maximum(0 , p_max + (rho)*dL(x_min,p_max))\n",
    "    return {'x_min': x_min, 'p_max': p_max, 'l_min': Lagragian1(x_min, p_max)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.zeros(10)\n",
    "results = uzawa(Lagrangian=Lagragian1, dL=dLagrangian1 , p0=p0 , x0=x0, rho=1, iterations=1000)\n",
    "x_min, l_min = results['x_min'], results['l_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrangian: argmaxmin = [-0.       0.12689 -0.       0.01941 -0.     ] l_min = -0.13853 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lagrangian: argmaxmin =', np.round(x_min, 5), 'l_min =', np.round(l_min, 5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our claims are confirmed and there is no duality gap. the solution for the dual problem matches the first solutions we got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Non Convex Optimisation : Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're gonna tackle a non convex problem, and try the simulated annealing algorithm to deal with it.\n",
    "\n",
    " We define $f_3$ wich is clearly a non convex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(U) : \n",
    "    return f1(U,B,S) + 10*np.sin(2*f1(U,B,S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a convex optimisation algorithm on this function with different starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for try 1/10 we get f_min: -10.7979\n",
      "for try 2/10 we get f_min: -1.37312\n",
      "for try 3/10 we get f_min: -1.37312\n",
      "for try 4/10 we get f_min: 14.33484\n",
      "for try 5/10 we get f_min: 4.91006\n",
      "for try 6/10 we get f_min: 26.90121\n",
      "for try 7/10 we get f_min: 55.17554\n",
      "for try 8/10 we get f_min: -1.37312\n",
      "for try 9/10 we get f_min: 36.32599\n",
      "for try 10/10 we get f_min: 11.19325\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "for i in range(10) : \n",
    "    x_init = rng.uniform(-1,1,5)\n",
    "    print(\"for try {}/10 we get f_min: {}\".format(i+1, np.round(minimize(fun = f3, x0 = x_init, method='BFGS', tol=1e-6)['fun'],5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the algorithm converges to different local minimas but we can't confirm that the minimum of them all isn't a global minima. This can say that in the ***general case***, the algorithm converges to a local minima.\n",
    "\n",
    "Let's try the simulated annealing which is more suitable to non convex  problems.\n",
    "\n",
    "In scipy, the simulated annealing function is deprecated and they propose a better algorithm under the name of basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for try 1/5 we get f_min : -10.7979 argmin : [-0.09805  0.00075 -0.43012  0.36459  0.34732]\n",
      "for try 2/5 we get f_min : -10.7979 argmin : [-0.8726   0.07141 -0.62244  0.73916 -0.04299]\n",
      "for try 3/5 we get f_min : -10.7979 argmin : [-0.50165  0.12506 -0.31614  0.1096  -0.08979]\n",
      "for try 4/5 we get f_min : -10.7979 argmin : [ 0.05298 -0.01769 -0.26537  0.12998  0.39975]\n",
      "for try 5/5 we get f_min : -10.7979 argmin : [-0.51921  0.11437 -0.80039  0.68051  0.24571]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "for i in range(5) : \n",
    "    x_init = rng.uniform(-1,1,5)\n",
    "    results = basinhopping(f3, x0 )\n",
    "    print(\"for try {}/5 we get f_min : {}\".format(i+1,np.round(results['fun'],5)), 'argmin :', np.round(results['x'],5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we always converge to the same minima, which is the ***best minima*** we got so far, but not the the same argmin due to the fact that our function is undulating. that means that the algorithm explores well the space of interest and ends up giving us the best minima we got (probably **the global minima**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Application to signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $H_0$ the ideal frequency response to be : $$ H_0(\\nu) = 1 \\quad for \\quad \\nu \\in [0, 0.1] \\quad, \\quad 0 \\quad \\quad for \\quad \\nu \\in [0.15, 0.5]$$\n",
    " \n",
    "and try to approximate it with an even function $H$ defined as follow:\n",
    "$$H(\\nu) = \\sum_{i=0}^{n}h[i]\\cos(2\\pi \\nu i)$$\n",
    "\n",
    "To do so, we try a discretisation of both frequencies intervals $\\{v_j\\}_{1\\leq j \\leq p}$ and define a criterion to minimize over all $h[i]$ :\n",
    "$$J(h) = \\max\\limits_{j}\\left| H_0(\\nu_j) - H(\\nu_j)\\right|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = np.vectorize(lambda v : 1 if (v >= 0 and v<= 0.1) else 0 if (v >= 0.15 and v<= 0.5) else None)\n",
    "\n",
    "#h must be a (30,) dimension array\n",
    "H = np.vectorize(lambda h,v : np.sum(h*np.cos(2*np.pi*v*np.arange(30))))\n",
    "H.excluded.add(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intervals have a length ratio of **3:8**, so for the sake of simplicity, we're gonna take 65 points for both intervals (15 for the first and 40 for the second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.concatenate((np.linspace(0, 0.1, 15),np.linspace(0.15, 0.5, 40)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the criterion to minimize\n",
    "J = lambda h : np.max(np.abs(H0(v) - H(h,v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for try 1/10 we get f_min: 0.35178248323312844\n",
      "for try 2/10 we get f_min: 4.265804938070777\n",
      "for try 3/10 we get f_min: 0.6964014679287449\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "for i in range(3) : \n",
    "    x0 = rng.normal(size = 30)\n",
    "    results = minimize(J, x0, method='BFGS', tol=1e-6)\n",
    "    print(\"for try {}/10 we get f_min: {}\".format(i+1,results['fun']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for try 1/10 we get f_min: 5.501589932460395\n",
      "for try 2/10 we get f_min: 4.8510053676540785\n",
      "for try 3/10 we get f_min: 3.6155587046744317\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "for i in range(3) : \n",
    "    x0 = rng.normal(size = 30)\n",
    "    results = minimize(J, x0, method='Nelder-Mead', tol=1e-6)\n",
    "    print(\"for try {}/10 we get f_min: {}\".format(i+1,results['fun']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the algorithm can't find a global minima, it gets stuck in local minimas. \n",
    "it takes more time then on the other functions as this one isn't regular enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the problem as a linear one with constraints. \n",
    "$$ \\min\\limits_{h} \\quad \\max\\limits_{j}\\left| H_0(\\nu_j) - H(\\nu_j)\\right| $$\n",
    "which then becomes : \n",
    "$$ \\min\\limits_{(h,z)} z  $$\n",
    "Such that $$   \\left| H_0(\\nu_j) - H(\\nu_j)\\right| \\le z    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
